Aim:
To implement the k-Nearest Neighbour (k-NN) algorithm to classify data from iris.csv and print correct/wrong predictions and the confusion matrix.

---

Algorithm (Short):

1. Load the dataset using Pandas.
2. Convert class labels into numeric using LabelEncoder.
3. Split the dataset into train and test sets.
4. Train the k-NN classifier with k=3.
5. Predict labels for test data.
6. Display predictions and confusion matrix.

---

Dataset (iris.csv):

Save this as iris.csv in your working folder or download from internet:

sepal_length,sepal_width,petal_length,petal_width,species
5.1,3.5,1.4,0.2,setosa
4.9,3.0,1.4,0.2,setosa
6.2,2.2,4.5,1.5,versicolor
5.9,3.0,5.1,1.8,virginica
6.0,2.2,5.0,1.5,virginica
6.1,2.9,4.7,1.4,versicolor
...

(Add all 150 records from the UCI dataset for full experiment. This is just a preview.)

---

Program (Very Short):

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import confusion_matrix
from sklearn.preprocessing import LabelEncoder

d = pd.read_csv("iris.csv")
X, y = d.iloc[:, :-1], LabelEncoder().fit_transform(d.iloc[:, -1])
X_tr, X_te, y_tr, y_te = train_test_split(X, y, test_size=0.3)

m = KNeighborsClassifier(n_neighbors=3).fit(X_tr, y_tr)
p = m.predict(X_te)

[print(f"P:{i}, A:{j}", "✔" if i==j else "✘") for i,j in zip(p, y_te)]
print("Confusion Matrix:\n", confusion_matrix(y_te, p))

---

Sample Output:

P:0, A:0 ✔  
P:1, A:1 ✔  
P:2, A:2 ✔  
P:1, A:2 ✘  
...
Confusion Matrix:
[[14  0  0]
 [ 0 12  1]
 [ 0  2 16]]


---

Result:
The k-NN algorithm was successfully implemented using the iris.csv dataset. Predictions were printed and evaluated using a confusion matrix.
